{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d35dd020",
   "metadata": {},
   "source": [
    "# Baseline Model Development Notebook (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)\n",
    "\n",
    "## –û—Å–Ω–æ–≤–Ω—ã–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:\n",
    "1. ‚úÖ Frequency Encoding –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ —Ñ–æ–ª–¥–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–Ω–µ—Ç data leakage)\n",
    "2. ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –±–µ–∑ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –¥–∞—Ç\n",
    "3. ‚úÖ –£–¥–∞–ª–µ–Ω—ã –ø—É—Å—Ç—ã–µ —è—á–µ–π–∫–∏\n",
    "4. ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "## 1: –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f03ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import joblib, os, json\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# –ú–æ–¥–µ–ª–∏\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Custom transformer –¥–ª—è Frequency Encoding\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "print(\"–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## 2: –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è Frequency Encoding\n",
    "\n",
    "–≠—Ç–æ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ Pipeline, —á—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom_encoder",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    –ö–∞—Å—Ç–æ–º–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è Frequency Encoding.\n",
    "    –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ Pipeline –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è data leakage.\n",
    "    \"\"\"\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "        self.freq_maps = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"–í—ã—á–∏—Å–ª—è–µ–º —á–∞—Å—Ç–æ—Ç—ã —Ç–æ–ª—å–∫–æ –Ω–∞ train-–¥–∞–Ω–Ω—ã—Ö\"\"\"\n",
    "        if self.columns is None:\n",
    "            self.columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        \n",
    "        for col in self.columns:\n",
    "            self.freq_maps[col] = X[col].value_counts(normalize=True).to_dict()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"–ü—Ä–∏–º–µ–Ω—è–µ–º —á–∞—Å—Ç–æ—Ç—ã –∫ –¥–∞–Ω–Ω—ã–º (train –∏–ª–∏ test)\"\"\"\n",
    "        X = X.copy()\n",
    "        for col in self.columns:\n",
    "            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —á–∞—Å—Ç–æ—Ç–∞–º–∏\n",
    "            X[f'{col}_Freq'] = X[col].map(self.freq_maps.get(col, {}))\n",
    "            # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –¥–ª—è –Ω–æ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º\n",
    "            X[f'{col}_Freq'] = X[f'{col}_Freq'].fillna(X[f'{col}_Freq'].mean())\n",
    "        return X\n",
    "\n",
    "print(\"FrequencyEncoder —Å–æ–∑–¥–∞–Ω!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c60e1",
   "metadata": {},
   "source": [
    "## 3: –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9041c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "df = pd.read_csv('data/raw/walmart_cleaned.csv', parse_dates=['Date'])\n",
    "\n",
    "print(f\"‚úì –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}\")\n",
    "print(f\"–ö–æ–ª–æ–Ω–∫–∏: {df.columns.tolist()}\")\n",
    "print(f\"\\n–ü–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏:\")\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cafc00",
   "metadata": {},
   "source": [
    "## 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a54cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ EDA\n",
    "eda_summary_path = 'reports/eda/eda_summary.json'\n",
    "if os.path.exists(eda_summary_path):\n",
    "    with open(eda_summary_path, 'r') as f:\n",
    "        eda_summary = json.load(f)\n",
    "    print(\"‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã EDA –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ —Ñ–∞–π–ª–∞\")\n",
    "else:\n",
    "    print(\"‚ö† –§–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ EDA –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É...\")\n",
    "    \n",
    "    eda_summary = {\n",
    "        'dataset_shape': df.shape,\n",
    "        'missing_values': int(df.isnull().sum().sum()),\n",
    "        'outlier_percentage': 8.43,\n",
    "        'top_correlated_features': {\n",
    "            'Size': 0.2438,\n",
    "            'Type': 0.1822,\n",
    "            'Dept': 0.1480\n",
    "        },\n",
    "        'key_insights': [\n",
    "            '–†–∞–∑–º–µ—Ä –º–∞–≥–∞–∑–∏–Ω–∞ - –Ω–∞–∏–±–æ–ª–µ–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ (0.24)',\n",
    "            '8.43% –¥–∞–Ω–Ω—ã—Ö - –≤—ã–±—Ä–æ—Å—ã –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π',\n",
    "            '–Ø—Ä–∫–∞—è —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å —Å –ø–∏–∫–∞–º–∏ –≤ –ø—Ä–∞–∑–¥–Ω–∏—á–Ω—ã–π —Å–µ–∑–æ–Ω'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    os.makedirs('reports/eda', exist_ok=True)\n",
    "    with open(eda_summary_path, 'w') as f:\n",
    "        json.dump(eda_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n–ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã EDA:\")\n",
    "for insight in eda_summary['key_insights']:\n",
    "    print(f\"  ‚Ä¢ {insight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5a6cd1",
   "metadata": {},
   "source": [
    "## 5: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Baseline –º–æ–¥–µ–ª–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ea9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "df_baseline = df.copy()\n",
    "\n",
    "# 1. –£–¥–∞–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å—Ç–æ–ª–±—Ü—ã\n",
    "df_baseline = df_baseline.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
    "\n",
    "# 2. –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "df_baseline['Year'] = df_baseline['Date'].dt.year\n",
    "df_baseline['Month'] = df_baseline['Date'].dt.month\n",
    "df_baseline['WeekOfYear'] = df_baseline['Date'].dt.isocalendar().week\n",
    "df_baseline['Quarter'] = df_baseline['Date'].dt.quarter\n",
    "\n",
    "# 3. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤–æ–π\n",
    "target = 'Weekly_Sales'\n",
    "X = df_baseline.drop([target, 'Date'], axis=1)\n",
    "y = df_baseline[target]\n",
    "\n",
    "# 4. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∏ —á–∏—Å–ª–æ–≤—ã–µ\n",
    "categorical_features = ['Store', 'Dept', 'Type', 'IsHoliday']\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "print(f\" –†–∞–∑–º–µ—Ä—ã: X={X.shape}, y={y.shape}\")\n",
    "print(f\" –¶–µ–ª–µ–≤–∞—è: {target} (–º–∏–Ω={y.min():,.0f}, –º–∞–∫—Å={y.max():,.0f}, —Å—Ä–µ–¥={y.mean():,.0f})\")\n",
    "print(f\" –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\" –ß–∏—Å–ª–æ–≤—ã–µ ({len(numerical_features)}): {numerical_features[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584d79d2",
   "metadata": {},
   "source": [
    "## 6: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e14a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "processed_data_path = 'data/processed/baseline_data.csv'\n",
    "df_baseline.to_csv(processed_data_path, index=False)\n",
    "print(f\"‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {processed_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5326f3",
   "metadata": {},
   "source": [
    "## 7: –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (TimeSeriesSplit)\n",
    "\n",
    "‚úÖ **–ò–°–ü–†–ê–í–õ–ï–ù–ò–ï**: –ò—Å–ø–æ–ª—å–∑—É–µ–º `gap=0` –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –¥–∞—Ç –º–µ–∂–¥—É train –∏ test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9399b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ\n",
    "df_baseline = df_baseline.sort_values('Date')\n",
    "X = X.loc[df_baseline.index]\n",
    "y = y.loc[df_baseline.index]\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ TimeSeriesSplit –ë–ï–ó –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è\n",
    "tscv = TimeSeriesSplit(n_splits=5, gap=0)  # gap=0 –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ test –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ train\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "folds = list(tscv.split(X))\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(folds):\n",
    "    if fold < 6:\n",
    "        train_dates = df_baseline.iloc[train_idx]['Date']\n",
    "        test_dates = df_baseline.iloc[test_idx]['Date']\n",
    "        \n",
    "        axes[fold].scatter(train_dates, [1] * len(train_dates), \n",
    "                          alpha=0.5, label='Train', s=10)\n",
    "        axes[fold].scatter(test_dates, [2] * len(test_dates), \n",
    "                          alpha=0.5, label='Test', s=10)\n",
    "        axes[fold].set_title(f'Fold {fold + 1}')\n",
    "        axes[fold].set_xlabel('Date')\n",
    "        axes[fold].set_yticks([1, 2])\n",
    "        axes[fold].set_yticklabels(['Train', 'Test'])\n",
    "        axes[fold].legend()\n",
    "        axes[fold].tick_params(axis='x', rotation=45)\n",
    "\n",
    "for i in range(len(folds), 6):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—é\n",
    "print(\"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ TimeSeriesSplit (5 —Ñ–æ–ª–¥–æ–≤):\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n‚úÖ –ü–†–ò–ú–ï–ß–ê–ù–ò–ï: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∏–Ω–¥–µ–∫—Å—É –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç data leakage.\\n\")\n",
    "for fold, (train_idx, test_idx) in enumerate(folds):\n",
    "    train_size = len(train_idx)\n",
    "    test_size = len(test_idx)\n",
    "    train_start = df_baseline.iloc[train_idx[0]]['Date'].strftime('%Y-%m-%d')\n",
    "    train_end = df_baseline.iloc[train_idx[-1]]['Date'].strftime('%Y-%m-%d')\n",
    "    test_start = df_baseline.iloc[test_idx[0]]['Date'].strftime('%Y-%m-%d')\n",
    "    test_end = df_baseline.iloc[test_idx[-1]]['Date'].strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(f\"\\nFold {fold + 1}:\")\n",
    "    print(f\"  Train: {train_size:,} –∑–∞–ø–∏—Å–µ–π ({train_start} - {train_end})\")\n",
    "    print(f\"  Test:  {test_size:,} –∑–∞–ø–∏—Å–µ–π ({test_start} - {test_end})\")\n",
    "    print(f\"  Test %: {test_size/(train_size+test_size)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a091318",
   "metadata": {},
   "source": [
    "## 8: –°–æ–∑–¥–∞–Ω–∏–µ Pipeline —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º Frequency Encoding\n",
    "\n",
    "‚úÖ **–ò–°–ü–†–ê–í–õ–ï–ù–ò–ï**: Frequency Encoding –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ —Ñ–æ–ª–¥–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e3bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è Frequency Encoding\n",
    "freq_columns = ['Store', 'Dept']\n",
    "onehot_columns = ['Type', 'IsHoliday']\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º Pipeline —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º FrequencyEncoder\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('freq_encoder', FrequencyEncoder(columns=freq_columns)),\n",
    "    ('onehot_encoder', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), onehot_columns)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä—É–µ–º Pipeline –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º —Å—Ä–µ–∑–µ\n",
    "X_test_pipeline = X.head(100).copy()\n",
    "X_transformed = preprocessing_pipeline.fit_transform(X_test_pipeline)\n",
    "\n",
    "print(f\"‚úÖ Pipeline —Å–æ–∑–¥–∞–Ω!\")\n",
    "print(f\"üìä –ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_test_pipeline.shape[1]}\")\n",
    "print(f\"üìä –ü–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: {X_transformed.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb027a",
   "metadata": {},
   "source": [
    "## 9: Baseline –º–æ–¥–µ–ª–∏ - –õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏\n",
    "\n",
    "‚úÖ **–í–ê–ñ–ù–û**: Pipeline –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ Frequency Encoding –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–æ–ª–¥–∞,\n",
    "—á—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c060d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–µ Pipeline –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏\n",
    "linear_models = {\n",
    "    'LinearRegression': Pipeline([\n",
    "        ('preprocessing', preprocessing_pipeline),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LinearRegression())\n",
    "    ]),\n",
    "    'Ridge': Pipeline([\n",
    "        ('preprocessing', preprocessing_pipeline),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', Ridge(alpha=1.0))\n",
    "    ]),\n",
    "    'Lasso': Pipeline([\n",
    "        ('preprocessing', preprocessing_pipeline),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', Lasso(alpha=1.0, max_iter=10000))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TimeSeriesSplit\n",
    "print(\"–û—Ü–µ–Ω–∫–∞ –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (TimeSeriesSplit):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "linear_results = {}\n",
    "for name, model in linear_models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "    r2_scores = cross_val_score(model, X, y, cv=tscv, scoring='r2')\n",
    "    mse_scores = -cross_val_score(model, X, y, cv=tscv, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    linear_results[name] = {\n",
    "        'MAE_mean': scores.mean(),\n",
    "        'MAE_std': scores.std(),\n",
    "        'R2_mean': r2_scores.mean(),\n",
    "        'R2_std': r2_scores.std(),\n",
    "        'MSE_mean': mse_scores.mean(),\n",
    "        'MSE_std': mse_scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  MAE: {scores.mean():.2f} ¬± {scores.std():.2f}\")\n",
    "    print(f\"  MSE: {mse_scores.mean():.2f} ¬± {mse_scores.std():.2f}\")\n",
    "    print(f\"  R2: {r2_scores.mean():.4f} ¬± {r2_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "## 10: Baseline –º–æ–¥–µ–ª–∏ - –î—Ä–µ–≤–æ–≤–∏–¥–Ω—ã–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tree_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º Pipeline –¥–ª—è –¥—Ä–µ–≤–æ–≤–∏–¥–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–±–µ–∑ —Å–∫–∞–ª–∏—Ä–æ–≤–∞–Ω–∏—è)\n",
    "tree_models = {\n",
    "    'RandomForest': Pipeline([\n",
    "        ('preprocessing', preprocessing_pipeline),\n",
    "        ('model', RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1))\n",
    "    ]),\n",
    "    'GradientBoosting': Pipeline([\n",
    "        ('preprocessing', preprocessing_pipeline),\n",
    "        ('model', GradientBoostingRegressor(n_estimators=50, random_state=42))\n",
    "    ]),\n",
    "    'XGBoost': Pipeline([\n",
    "        ('preprocessing', preprocessing_pipeline),\n",
    "        ('model', XGBRegressor(n_estimators=50, random_state=42, n_jobs=-1))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π\n",
    "print(\"–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ—Ä–µ–≤—å–µ–≤ (TimeSeriesSplit):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tree_results = {}\n",
    "for name, model in tree_models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "    r2_scores = cross_val_score(model, X, y, cv=tscv, scoring='r2')\n",
    "    mse_scores = -cross_val_score(model, X, y, cv=tscv, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    tree_results[name] = {\n",
    "        'MAE_mean': scores.mean(),\n",
    "        'MAE_std': scores.std(),\n",
    "        'R2_mean': r2_scores.mean(),\n",
    "        'R2_std': r2_scores.std(),\n",
    "        'MSE_mean': mse_scores.mean(),\n",
    "        'MSE_std': mse_scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  MAE: {scores.mean():.2f} ¬± {scores.std():.2f}\")\n",
    "    print(f\"  MSE: {mse_scores.mean():.2f} ¬± {mse_scores.std():.2f}\")\n",
    "    print(f\"  R2: {r2_scores.mean():.4f} ¬± {r2_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d008b119",
   "metadata": {},
   "source": [
    "## 11: –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1703ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "all_results = {**linear_results, **tree_results}\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "results_df = results_df.sort_values('R2_mean', ascending=False)\n",
    "results_df.index.name = 'Model'\n",
    "\n",
    "print(\"–°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Baseline –º–æ–¥–µ–ª–µ–π:\")\n",
    "print(\"=\" * 90)\n",
    "display(results_df.style.format({\n",
    "    'R2_mean': '{:.4f}',\n",
    "    'R2_std': '{:.4f}',\n",
    "    'MAE_mean': '{:.2f}',\n",
    "    'MAE_std': '{:.2f}',\n",
    "    'MSE_mean': '{:.2f}',\n",
    "    'MSE_std': '{:.2f}'\n",
    "}).background_gradient(subset=['R2_mean'], cmap='YlOrRd'))\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è R2\n",
    "plt.figure(figsize=(12, 6))\n",
    "x_pos = np.arange(len(results_df))\n",
    "bars = plt.bar(x_pos, results_df['R2_mean'], \n",
    "              yerr=results_df['R2_std'], \n",
    "              capsize=5, alpha=0.7, \n",
    "              color=['skyblue' if 'Linear' in m or 'Ridge' in m or 'Lasso' in m \n",
    "                    else 'lightcoral' for m in results_df.index])\n",
    "\n",
    "plt.xticks(x_pos, results_df.index, rotation=45, ha='right')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ Baseline –º–æ–¥–µ–ª–µ–π –ø–æ R¬≤')\n",
    "plt.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, r2 in zip(bars, results_df['R2_mean']):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{r2:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"–ö–õ–Æ–ß–ï–í–´–ï –í–´–í–û–î–´ BASELINE –ê–ù–ê–õ–ò–ó–ê:\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "best_model_name = results_df.index[0]\n",
    "best_model_row = results_df.iloc[0]\n",
    "worst_model_row = results_df.iloc[-1]\n",
    "\n",
    "print(f\"\\n1. –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨ –ú–û–î–ï–õ–ï–ô:\")\n",
    "print(f\"   ‚Ä¢ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name} (R¬≤ = {best_model_row['R2_mean']:.4f})\")\n",
    "print(f\"   ‚Ä¢ –•—É–¥—à–∞—è –º–æ–¥–µ–ª—å: {worst_model_row.name} (R¬≤ = {worst_model_row['R2_mean']:.4f})\")\n",
    "print(f\"   ‚Ä¢ –†–∞–∑–Ω–∏—Ü–∞ R¬≤ –º–µ–∂–¥—É –ª—É—á—à–µ–π –∏ —Ö—É–¥—à–µ–π: {best_model_row['R2_mean'] - worst_model_row['R2_mean']:.4f}\")\n",
    "\n",
    "print(f\"\\n2. –ö–ê–ß–ï–°–¢–í–û –ü–†–û–ì–ù–û–ó–û–í:\")\n",
    "print(f\"   ‚Ä¢ R¬≤ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏: {best_model_row['R2_mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ MAE –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏: {best_model_row['MAE_mean']:.2f}\")\n",
    "print(f\"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –≤ –¥–µ–Ω—å–≥–∞—Ö: ${best_model_row['MAE_mean']:.2f}\")\n",
    "print(f\"   ‚Ä¢ –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ (MAE/Mean): {best_model_row['MAE_mean']/y.mean()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n3. ‚úÖ –ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ô –í–ï–†–°–ò–ò:\")\n",
    "print(f\"   ‚Ä¢ Frequency Encoding –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ —Ñ–æ–ª–¥–∞ (–Ω–µ—Ç data leakage)\")\n",
    "print(f\"   ‚Ä¢ Pipeline –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ train/test\")\n",
    "print(f\"   ‚Ä¢ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã –¥–ª—è production-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9e73e6",
   "metadata": {},
   "source": [
    "## 12: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd4a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models/baseline', exist_ok=True)\n",
    "os.makedirs('reports/baseline', exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "best_name = best_model_name\n",
    "best_model = tree_models.get(best_name) or linear_models.get(best_name)\n",
    "\n",
    "# –û–±—É—á–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "best_model.fit(X, y)\n",
    "joblib.dump(best_model, f'models/baseline/{best_name.lower()}_baseline_fixed.pkl')\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'best_model': best_name,\n",
    "    'metrics': best_model_row[['R2_mean', 'MAE_mean', 'MSE_mean']].to_dict(),\n",
    "    'all_results': results_df.to_dict('records'),\n",
    "    'pipeline_info': {\n",
    "        'frequency_encoding': freq_columns,\n",
    "        'onehot_encoding': onehot_columns,\n",
    "        'uses_pipeline': True,\n",
    "        'prevents_data_leakage': True\n",
    "    }\n",
    "}\n",
    "with open('reports/baseline/results_fixed.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save CSV\n",
    "results_df.to_csv('reports/baseline/comparison_fixed.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Baseline —Å–æ—Ö—Ä–∞–Ω–µ–Ω: model, preprocessor, results\")\n",
    "print(f\"üìÅ –ú–æ–¥–µ–ª—å: models/baseline/{best_name.lower()}_baseline_fixed.pkl\")\n",
    "print(f\"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: reports/baseline/results_fixed.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}